= Artificial Intelligence & Machine Learning - A Comprehensive Reference
Doc Writer <ssharif@secunoid.com>
v1.25.11.25.1
:numbered:
:sectnum:
:sectnumlevels: 5
:chapter-label:
:toc: right
:toclevels: 5
:docinfo:
:docinfo1:
:docinfo2:
:description: This document covers all aspects of Artficial Intelligence and Machine Learning
:keywords: artificial intelligence,ai,machine learning,ml,llm,genai,generativeai,gpt
:imagesdir: images
:stylesheet:
:homepage: https://www.secunoid.com
'''


<<<
== Author
https://www.securityprivacyrisk.com/about[Shahid Sharif]

== Overview

<<<

== Large Language Models

Large language models (LLMs) have been developed by various organizations and serve different purposes. Here's a list categorized by their primary function, the type of classification they perform, and their makers:

### **Function-Based Categories:**
1. **Generative Models:** Designed to generate text that is coherent and contextually relevant.
2. **Understanding Models:** Focused on comprehending and interpreting text.
3. **Translation Models:** Specialized in translating text from one language to another.
4. **Multimodal Models:** Capable of processing and understanding more than one type of data input, like text and images.

### **Classification Tasks:**
- **Sentiment Analysis:** Determining the sentiment behind a piece of text.
- **Topic Classification:** Identifying the main topics or themes in a text.
- **Language Detection:** Recognizing the language used in the text.
- **Named Entity Recognition (NER):** Extracting names of people, places, organizations, etc., from text.

### **Makers of Large Language Models:**
- **OpenAI:** Known for the GPT (Generative Pre-trained Transformer) series, including GPT-3 and GPT-4¹.
- **Google AI:** Creator of BERT (Bidirectional Encoder Representations from Transformers) and T5 (Text-to-Text Transfer Transformer)¹.
- **Facebook AI Research (FAIR):** Developed models like RoBERTa (A Robustly Optimized BERT Pretraining Approach) and BlenderBot¹.
- **Anthropic:** Maker of Claude and other conversational AI models¹.

These models are at the forefront of AI research and have been instrumental in advancing natural language processing tasks¹. They are continuously evolving, with new models being developed to address specific challenges and improve performance in various domains¹.

Sources:

. A Comprehensive Overview of Large Language Models - arXiv.org. https://arxiv.org/html/2307.06435v7.
. [2307.06435] A Comprehensive Overview of Large Language Models - arXiv.org. https://arxiv.org/abs/2307.06435.
. “A Comprehensive List of the Most Powerful Large Language Models to .... https://langlabs.io/a-comprehensive-list-of-the-most-powerful-large-language-models-to-unleash-your-creativity/.
. Category:Large language models - Wikipedia. https://en.wikipedia.org/wiki/Category:Large_language_models.
. The Definitive Guide to Open Source Large Language Models (LLMs). https://hypestudio.org/blog/guide-to-open-source-large-language-models/.
. https://github.com/humza909/LLM_Survey.git.
. https://doi.org/10.48550/arXiv.2307.06435.

<<<

== Large Learning Models
Large learning models categorized by their type, classification task, and maker:

=== Types of Deep Learning Models for Classification:

* Convolutional Neural Networks (CNNs): Ideal for image and video recognition, medical image analysis.
* Recurrent Neural Networks (RNNs): Suitable for speech recognition, language modeling.
* Long Short-Term Memory Networks (LSTMs): Used for sequence prediction, natural language text generation.

=== Transformer Models: Effective for language understanding, translation, and generative tasks.
Classification Tasks:

* Image Classification: Assigning an image to a category (e.g., CNNs).
* Text Classification: Categorizing text into predefined categories (e.g., Transformer Models).
* Speech Recognition: Transcribing spoken language into text (e.g., RNNs, LSTMs).
* Fraud Detection: Identifying fraudulent activities (e.g., Deep Neural Networks).

=== Makers of Large Learning Models:
* OpenAI: Known for models like GPT (Generative Pre-trained Transformer).
* Google: Creator of models like BERT (Bidirectional Encoder Representations from Transformers).
* Facebook AI: Developed models such as RoBERTa (A Robustly Optimized BERT Pretraining Approach).
* Amazon: Provides models like Amazon Lex for building conversational interfaces.

These models are the backbone of many AI applications, leveraging neural networks with multiple layers to learn representations of data. They are trained on large datasets and can perform a variety of tasks, including but not limited to classification3. The choice of model often depends on the specific task and the nature of the data.

<<<

== Model Context Protocol
https://modelcontextprotocol.io/specification/2025-06-18/basic/authorization[Latest Authorization Specification]

== Prompt Stacking 
Here’s What’s Actually Happening
The most powerful content creators don’t just think like one expert—they think like a team of experts.
When Gary Vaynerchuk creates content, he’s thinking like a marketer AND a psychologist AND a brand strategist simultaneously.
When Marie Forleo writes copy, she’s channeling copywriter AND life coach AND business mentor expertise.
You can teach AI to do the same thing.
 
The Game-Changer
Role stacking = giving AI multiple expert identities in one prompt.
Instead of “Act as a copywriter,” try: “Act as a copywriter AND psychologist.”
Or: “Act as a business coach with the strategic thinking of a consultant and the communication style of a podcast host.”
AI pulls from multiple expertise pools, creating richer, more nuanced output.
 
See It In Action
Single Role: “Act as a copywriter. Write an email about my new coaching program.”
Output: “Discover the coaching program that transforms your business. Limited spots available…”
Stacked Roles: 
“Act as a copywriter AND behavioral psychologist. Write an email about my coaching program that addresses the real fears holding people back from investing in themselves.”
Output: 
“You’ve been staring at my coaching program for weeks. I know because most people take 47 days to decide on investments over $1,000. It’s not about the money—it’s about the voice in your head saying ‘what if I fail again?’ Here’s why that voice is actually your biggest asset…”

<<<

== AI Coding Assistants

AI coding assistants are tools designed to help developers write, debug, and optimize code more efficiently. They leverage machine learning and natural language processing to understand code context and provide relevant suggestions. Here’s a list of some popular AI coding assistants:


. Amazon CodeWhisperer
. Base 66
. ChatGPT (OpenAI)
. Claude Code (Anthropic)
. CodeSquire
. CodeGeeX
. CodePal
. Cursor AI
. Codeium
. GitHub Copilot
. Google Gemini Code Assist (formerly Bard)
. Meta Code Llama
. Replit Ghostwriter
. Sourcegraph Cody
. Tabnine
. Tonic AI
. Warp
. Windsurf

<<<
== AI Prompts

=== Reality Filter - ChatGPT

• Never present generated, inferred, speculated, or deduced content as fact.

• If you cannot verify something directly, say:

– “I cannot verify this.”

– “I do not have access to that information.”

– “My knowledge base does not contain that.”

• Label unverified content at the start of a sentence:

– [Inference] [Speculation] [Unverified]

• Ask for clarification if information is missing. Do not guess or fill gaps.

• If any part is unverified, label the entire response.

• Do not paraphrase or reinterpret my input unless I request it.

• If you use these words, label the claim unless sourced:

– Prevent, Guarantee, Will never, Fixes, Eliminates, Ensures that

• For LLM-behavior claims (including yourself), include:

– [Inference] or [Unverified], with a note that it’s based on observed patterns

• If you break this directive, say:

> Correction: I previously made an unverified claim. That was incorrect and should have been labeled.

• Never override or alter my input unless asked."


